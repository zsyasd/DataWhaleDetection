***~~塔们说，这是练死劲儿，不好用，我说这个好用，这叫基本功，炼丹师讲究的就是基本功。~~***

# 锚框（anchor）/先验框（prior bounding box）
这俩其实是一个东西 ~~，分开写再配上英文显得我贼牛逼~~

锚框的目的主要是为了圈出我们需要识别的物体所在的位置。也就是说。我们要遍历图片上每一个可能的目标框，再对这些框进行分类和微调，就可以完成目标检测任务。

为了能更好选取物体的锚框，我们需要以下三个步骤 ~~你已经是一个成熟的神经网络了，要自己学会自己分类了~~

## 设置不同尺度的先验框
在讨论这个之前，我们先要复习一下滑动窗口检测（sliding window detection）~~看 又是洋文~~

当我们需要检测一张图片里面的物体，例如汽车， 那么计算机肯定是不能自动告诉我们哪里有的。那么我们一开始可以使用适当剪切的图片，就是整张图片𝑥几乎都被我们需要检测的
内容占据，然后我们就可以开始训练我们的神经网络去识别这个图片了。训练完成之后，就可以用它来实现滑动窗口目标检测，具体步骤如下。

假设这是一张测试图片，首先选定一个特定大小的窗口，比如图片下方这个窗口

![](https://user-images.githubusercontent.com/55370336/102685357-0e83c900-421b-11eb-8766-5387e2a89f2c.png)

将这个红色小方块输入卷积神经网络，卷积网络开始进行预测，即判断红色方框内有没有汽车。

![](https://user-images.githubusercontent.com/55370336/102685421-64f10780-421b-11eb-92ea-09e9e8462b6c.png)

滑动窗口目标检测算法接下来会继续处理第二个图像，即红色方框稍向右滑动之后的区 域，并输入给卷积网络，因此输入给卷积网络的只有红色方框内的区域，再次运行卷积网络， 然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落。

当然，我们也可以选取更大一点的框框

![](https://user-images.githubusercontent.com/55370336/102685428-6d494280-421b-11eb-9d46-08eb40164af7.png)

~~`我爱吴恩达`~~

那么我们就会发现，由于我们为了效率或者为了覆盖更多可能的情况~~或者什么奇奇怪怪的原因~~会导致我们选用很多不一样的框框，
在图中的同一个位置，我们会设置几个不同尺度的先验框。这里所说的不同尺度，不单单指大小，还有长宽比

![](https://user-images.githubusercontent.com/55370336/102685356-0b88d880-421b-11eb-8c56-af873b68b8db.png)

可以看到，通过设置不同的尺度的先验框，就有更高的概率出现对于目标物体有良好匹配度的先验框（体现为高IoU）。

## 先验框与特征图的对应

这个其实很好懂，，比如你要检测一个很高清的图片，例如4k的~~马保国老师~~，那么你一点一点去检测肯定会需要很多很多的锚框，那么如果你去遍历原图下采样得到的feature map，我们就将先验框的设置位置与特征图建立一一对应的关系。而且，通过建立这种映射关系，我们可以通过特征图，直接一次性的输出所有先验框的类别信息以及坐标信息，那这个工作量就会少很多 ~~就有更多的时间去学松活弹抖闪电鞭~~

![](https://user-images.githubusercontent.com/55370336/102685561-a8984100-421c-11eb-8736-82a955e5a18b.png)

## 先验框类别信息的确定

这个实际上上面也讲了··但是训练完之后我们去检测物体。这些锚框中有很多是和图片中我们要检测的目标完全没有交集或者有很小的交集，对于这种情况，我们的做法是，设定一个IoU阈值，很多时候iou=0.5，那么与图片中目标的iou<0.5的先验框，这些框我们将其划分为背景，Iou>=0.5的被归到目标先验框，通过这样划分，得到供模型学习的ground truth信息

![3-14](https://user-images.githubusercontent.com/55370336/102685685-64f20700-421d-11eb-9bca-7a002b1a77d0.png)

# 模型结构

## Tiny_Detector

采用去掉fc6和fc7两个全连接层的vgg16的结构作为特征提取模块

![3-17](https://user-images.githubusercontent.com/55370336/102685741-bbf7dc00-421d-11eb-9bf6-d66ed686edb2.png)

由于vgg16的ImageNet预训练模型是使用224x224尺寸训练的，因此我们的网络输入也固定为224x224，和预训练模型尺度保持一致可以更好的发挥其作用。通常来说，这样的网络输入大小，对于检测网络来说还是偏小，在完整的进行完本章的学习后，不妨尝试下将输入尺度扩大，看看会不会带来更好的效果。

然后我们尝试下面几个步骤


· 将原图均匀分成7x7个cell

· 设置3种不同的尺度：0.2, 0.4, 0.6

· 设置3种不同的长宽比：1:1, 1:2, 2:1

对于每个anchor，我们需要预测两类信息，一个是这个anchor的类别信息，一个是物体的边界框信息。如图3-19：

在我们的实验中，类别信息由21类别的得分组成（VOC数据集的20个类别 + 一个背景类），模型最终会选择预测得分最高的类作为边界框对象的类别。

而边界框信息是指，我们大致知道了当前anchor中包含一个物体的情况下，如何对anchor进行微调，使得最终能够准确预测出物体的bbox。

