~~***我前面还说基本功好用，我变卦了啊！这就是练死劲儿，炼丹师讲究的是化劲儿。你光知道模型的结构，以及模型最终会输出什么怎么够，你得懂得化劲儿，
通过合理的设置损失函数和一些相关的训练技巧，让模型向着正确的方向学习，从而预测出我们想要的结果。***~~

# 损失函数设计

## 匹配策略
在上一次我们已经讲到我们在检测物体的时候需要用锚框框住我们需要检测的物体，然后再去检测里面的物体究竟是啥。那么我们分配了许多prior bboxes让其预测类别和目标框信息。那我们就要先要知道每个prior bbox和哪个目标对应，从而才能判断预测的是否准确，从而将训练进行下去。

不同方法 ground truth boxes 与 prior bboxes 的匹配策略大致都是类似的，但是细节会有所不同。这里我们采用SSD中的匹配策略，具体如下：

1. 从ground truth box出发，寻找与每一个ground truth box有最大的jaccard overlap的prior bbox，这样就能保证每一个groundtruth box一定与一个prior bbox对应起来(jaccard overlap就是IOU，如图3-26所示，前面介绍过)。 反之，若一个prior bbox没有与任何ground truth进行匹配，那么该prior bbox只能与背景匹配，就是负样本。

![](https://user-images.githubusercontent.com/55370336/102883274-b94dee80-448a-11eb-97e6-d423fd32ce8d.png)

一个图片中ground truth是非常少的，而prior bbox却很多，如果仅按第一个原则匹配，很多prior bbox会是负样本，正负样本极其不平衡，所以需要第二个原则。

2. 从prior bbox出发，对剩余的还没有配对的prior bbox与任意一个ground truth box尝试配对，只要两者之间的jaccard overlap大于阈值（一般是0.5），那么该prior bbox也与这个ground truth进行匹配。这意味着某个ground truth可能与多个Prior box匹配，这是可以的。但是反过来却不可以，因为一个prior bbox只能匹配一个ground truth，如果多个ground truth与某个prior bbox的 IOU 大于阈值，那么prior bbox只与IOU最大的那个ground truth进行匹配。

注意：第二个原则一定在第一个原则之后进行，仔细考虑一下这种情况，如果某个ground truth所对应最大IOU的prior bbox小于阈值，并且所匹配的prior bbox却与另外一个ground truth的IOU大于阈值，那么该prior bbox应该匹配谁，答案应该是前者，首先要确保每个ground truth一定有一个prior bbox与之匹配。

用一个示例来说明上述的匹配原则：

![](https://user-images.githubusercontent.com/55370336/102883278-bbb04880-448a-11eb-9187-f476f0b89aa2.png)

图像中有7个红色的框代表先验框，黄色的是ground truths，在这幅图像中有三个真实的目标。按照前面列出的步骤将生成以下匹配项：

![](https://user-images.githubusercontent.com/55370336/102883287-be12a280-448a-11eb-88ff-06216547a06a.png)

##损失函数

损失函数或成本函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数，借此直观表示的一些"成本"与事件的关联。一个最佳化问题的目标是将损失函数最小化。

在目标检测任务中，将总体的目标损失函数定义为```定位损失（loc）```和```置信度损失（conf）```的加权和：

![](https://camo.githubusercontent.com/469c04431b6c0afa466568af3e2dda0c850549c4db0905d180564a26039b840b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f4c28782c632c6c2c67293d25323025354366726163253742312537442537424e253744284c5f253742636f6e6625374428782c63292b253230253543616c7068612532302532304c5f2537426c6f6325374428782c6c2c672929283129)

其中N是匹配到GT（Ground Truth）的prior bbox数量，如果N=0，则将损失设为0；而 α 参数用于调整confidence loss和location loss之间的比例，默认 α=1。

confidence loss是在多类别置信度(c)上的softmax loss，公式如下：

![](https://camo.githubusercontent.com/e8de81be889968fb24c956cdd2466d8f28707862688e6a75273ea229ba16e2bc/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436d617468726d2537424c2537445f2537422535436d617468726d253742636f6e66253744253744282535436d617468726d253742782537442c2532302535436d617468726d25374263253744293d2d25354373756d5f2537422535436d617468726d25374269253744253230253543696e2532302535436d617468726d25374250253744253230253543746578742532302537422532306f732532302537442537442535452537422535436d617468726d2537424e2537442537442532302535436d617468726d253742782537445f2537422535436d617468726d253742696a2537442537442535452537422535436d617468726d253742702537442537442532302535436c6f672532302535436c656674282535436861742537422535436d617468726d253742632537442537445f2537422535436d617468726d253742692537442537442535452537422535436d617468726d253742702537442537442535437269676874292d25354373756d5f2537422535436d617468726d25374269253744253230253543696e2532302535436d617468726d2537424e65672537442537442532302535436c6f672532302535436c656674282535436861742537422535436d617468726d253742632537442537445f2537422535436d617468726d25374269253744253744253545253742302537442535437269676874292532302535436d617468726d2537425725374425323025354374657874253230253742253230686572652532302537442532302535436861742537422535436d617468726d253742692537442537445f2537422535436d617468726d253742692537442537442535452537422535436d617468726d253742702537442537443d253543667261632537422535436578702532302535436c656674282535436d617468726d253742632537445f2537422535436d617468726d253742692537442537442535452537422535436d617468726d2537427025374425374425354372696768742925374425374225354373756d5f2537422535436d617468726d253742702537442537442532302535436578702532302535436c656674282535436d617468726d253742632537445f2537422535436d617468726d253742692537442537442535452537422535436d617468726d25374270253744253744253543726967687429253744283229)

其中i指代搜索框序号，j指代真实框序号，p指代类别序号，p=0表示背景。其中x_{ij}^p={1,0}中取1表示第i个prior bbox匹配到第 j 个GT box，而这个GT box的类别为 p 。C_{i}^p表示第i个搜索框对应类别p的预测概率。此处有一点需要关注，公式前半部分是正样本（Pos）的损失，即分类为某个类别的损失（不包括背景），后半部分是负样本（Neg）的损失，也就是类别为背景的损失。

而location loss（位置回归）是典型的smooth L1 loss

![1431608647491_ pic](https://user-images.githubusercontent.com/55370336/102899155-764d4480-44a5-11eb-8853-0a08f50e5305.jpg)

其中，l为预测框，g为ground truth。(cx,xy)为补偿(regress to offsets)后的默认框d的中心,(w,h)为默认框的宽和高。

## Hard negative mining

值得注意的是，一般情况下negative prior bboxes数量 >> positive prior bboxes数量，直接训练会导致网络过于重视负样本，预测效果很差。为了保证正负样本尽量平衡，我们这里使用SSD使用的在线难例挖掘策略(hard negative mining)，即依据confidience loss对属于负样本的prior bbox进行排序，只挑选其中confidience loss高的bbox进行训练，将正负样本的比例控制在positive：negative=1:3。其核心作用就是只选择负样本中容易被分错类的困难负样本来进行网络训练，来保证正负样本的平衡和训练的有效性。

举个例子：假设在这 441 个 prior bbox 里，经过匹配后得到正样本先验框P个，负样本先验框 441−P 个。将负样本prior bbox按照prediction loss从大到小顺序排列后选择最高的M个prior bbox。这个M需要根据我们设定的正负样本的比例确定，比如我们约定正负样本比例为1:3时。我们就取M=3P，这M个loss最大的负样本难例将会被作为真正参与计算loss的prior bboxes，其余的负样本将不会参与分类损失的loss计算。
